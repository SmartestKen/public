
    



----------------------------reinforcement learning-----------------------------




markov decision process
    series of (states,actions,rewards)


a reduce to b
    if algorithm of b, the can get polynomial time algrithm of a


policy (state -> action)
state value function (total expected reward given state)
state-action value function (total expected reward given state,action)

model
    transition function (state, action -> state at t+1)
    reward function (state,action -> expected reward at t+1)
    can both be derived from P(s',r|s,a)

partially observable markov decision process
    states not completely observed (i.e. states used in transition function is not completely observed by agent)




policy iteration
    (dynamic programming, assume model is known to expand the expectation) 
    iteratively, get improved V given current policy, get Q from V, get improved policy from current Q by argmax

Monte-Carlo Methods (TD(1), where 1 means \lambda = 1)
    learning by simulating till the end and back propagate
1-step TD (TD(0), where 0 means \lambda = 0)
    learning by choosing a random action (single step) and stochastically update using bellman eqn
n-step TD
    learning by choosing random actions (n step) and stochastically update using bellman eqn
TD(\lambda)
    mixing n-step TD for all n with a weight parameter \lambda
eligibility traces
    split the TD(\lambda) expression to enable an online algorithm (rather than waiting till the end of episode)

GAE(\gamma, \lambda)
    advantage function estimator with \gamma as discount rate and \lambda as weighting factor of various n-step advantage estimators (like the weighting factor for TD(\lambda)). used in policy gradient expression
TRPO
    policy gradient (but with a KL constraint) to prevent large policy update
# https://arxiv.org/pdf/1707.06347.pdf
PPO
    surrogate objective (without constraint) to prevent large policy update
    (in L^CLIP, clip along x axis (policy ratio), optimize along y axis (value of objective), hence the original CPI term is not redundant)
    
TD methods
    learning a fraction of recursion difference (from bellman)
    SARSA
        replace expectation by current policy
    Q-Learning
        replace expectation by max
    DQN
        used to replace tables in Q-learning. Bootstrap the experience tuples set and only periodically update.
        Its loss function attempt to minimize square of recursion differences, which replaces the update step in SARSA.
    !!!Double DQN
        eval difference according to frozen approximation, but select action using current approximation (instead of max)
    ???QT-OPT
    TD(lambda)
        use n steps (n constant) bellman recursion with lambda reward discount rate
    importance sampling
        convert the sampling policy of any expectation term by using ratio
dx = x*(dx/x) = x*d(ln(x))
function approximator given target value
    1. simply update towards the target value (like in SARSA)
    2. parametrize a function, get gradient of loss function with respect to the parameter and gradient descent (like in DQN)


 

# http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_4_policy_gradient.pdf
Policy Gradient (parametrized policy)
    given samples (either on-policy or off-policy), obtain the policy gradient (with baseline = average of all returns offset) of the loss function (expected return given current state)
    update parameter using stochastic gradient descent
    
    REINFORCE
        select a from current policy, update by replacing expected value with current return (require Monte Carlo, i.e., the entire trajectory)
    actor-critic
        update by replacing expected value with current value of Q, then update Q like in Q learning 
    A3C
        multiple agent update the same policy/value function
    ???Deep Deterministic Policy Gradient
    ???TRPO
    !!!Proximal Policy Optimization
        change objective so that maximize the quality with respect to the quality of original policy (potentially with an
        extra term to penalize large distance from original policy/theta)

# either directly estimate (using trajectories) or using bellman equation for recursion (recursion always to one time step)
# neural network -> approximate a function

    

boltzmann policy
    exploration based on softmax
softmax (layer)
    normalization (given a vector of numbers, new val = e^(old_val) divided by sum of all exponentials)

Tree search algorithm
    Minimax
        each move as a node (regardless self or enemy), recursively min at enemy's level and max at self level till leaves (which gives an actual score)
    alpha-beta pruning
        minimax with pruning
    # http://ccg.doc.gold.ac.uk/ccg_old/teaching/ludic_computing/ludic16.pdf
    UCT (MCTS)
        push to boundary by UCB and expand the boundary to include a new child C, play and back-propagate Q value function (to everything from root to child C)
        at the end select the best child of root (UCB without second term)

    ???Smooth UCT
    ???B*
    Thompson sampling
        sample an MDP from history posterior, solve optimal Q and use corresponding greedy policy
    


# https://www.biostat.wisc.edu/~craven/cs760/lectures/AlphaZero.pdf
AlphaZero

# http://researchers.lille.inria.fr/~munos/papers/files/part1.pdf
policy iteration
    DP+Bellman (assume model known)
value iteration
    DP+Bellman (assume model known)
    Q-learning (MC/TD/TD(lambda)) + Bellman (assume model unknown)
policy gradient

Large Dim (-> use parametrization approximation rather than directly update a function (S,A table))
Approximate Value Iteration (DQN)
    V_{k+1} = min || V - TV_{k} ||
    setup a norm as distance function for bellman residual of the current approximation. The new approximation shall minimize this distance (and stay in network)

# http://researchers.lille.inria.fr/~munos/papers/files/part2.pdf    
Approximate Policy Iteration
    doing the same thing as AVI except using the current policy to sample (rather than max). Then use the new approximation to derive new policy.

! always think about state/action.rewad when looking at a RL problem

reward
    discount rate = 1 -> all rewards equally important
    discount rate  = 0 -> greedy (closest reward only)


convolution/convolutional layer
    e.g. kernel size  = 3 -> 3x3 matrix kernel (sometimes also directly called 3x3 convolutional layer)
    
    with this matrix kernel, at each location, perform elementwise multiplication between input and kernel, then sum up to give a scalar output
    
    stride 
        distance for every kernel move
        
    # https://stackoverflow.com/questions/42883547/intuitive-understanding-of-1d-2d-and-3d-convolutions-in-convolutional-neural-n
    conv1d, conv2d, conv3d convolution along 1,2,3 dimensions 
    
    
    https://stackoverflow.com/questions/27728531/convolutional-neural-networks-multiple-channels
    multi input/output channel
        each input node is a "graph" and each output node is a "graph". The kernel (as parameters between each input and output) sweep through that input and each kernel multiplication result in a single pixel (or part of the sum for a single pixel for multi input case) in the output graph.
    
        each input/output pair has a different kernel (as learnable parameter) between them. output_j = sum(input_k \ast kernel_kj)


        
Generalized Advantage Estimation ??-> value function

PPO
Early-stopping
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
deepcubeA
    A* search uses the current value function as heuristic to generate training batches. Then NN uses the batches to improve value function approximation.
    
    ! The search algorithm must fit the training goal (e.g. shortest path to reward/termination state)
    ! During testing, use heuristic (trained network) only (i.e. gbfs, greedy best first search)

gradient descent 
    learning rate is always the nominator on the gradient term at update eqn
    
https://stackoverflow.com/questions/34244452/whats-the-difference-between-best-first-search-and-a-search
gbfs (greedy best first search)
    consider only the heuristic
A*
    consider the sum of cost_from_start and heuristic
    A* always select node with the smallest f from the open SET
    A* update if f_child < g_current + 1 + h(child)
    reduces to g_child < g_current + 1 (dijkstra). In other words, the internal update is the same except A* uses f (rather than g) to choose nodes from open

    
string as the ultimate hashing solution    
neural network training
    training batch (x,y)
    forward pass with x and compute gradient of L(f(x,theta),y) using back propagation
    gradient descent on theta
    
    ! while thinking about state encoding, need to care about action coding complexity





pytorch
    ! unparametrized function appro may fail when seeing state far away from the training states, but parametrized function appro will not

    ! searching collect experience in a form that will utilize by NN's criterion

    ! [:,0] for any member, extract its first item and put them into a tensor
    [0,:] extract everything from the first member and put them into a tensor
    !! force positive at the end may create bad loss function

    ! convert to tensor and put them into criterion 
    ! tensor notation (a, b, c, d....), a sequence of number means dimensions from outmost ot innermost

    ! expecting two dimensional float tensors, first dimension -> batch size, second_dimension -> each input in batch
    ! ouput data can extracted using [:, 0] from what forward() returns


    ! need to return x at the end of forward()
    ! frozen = off-policy = train nn by batches from a frozen copy of the network. Then make frozen parameters up to date at some point (and repeat this process)
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    


# https://web.stanford.edu/class/archive/cs/cs103/cs103.1134/lectures/10/Small10.pdf    
DPLL (clause will be removed if eval to true, and leave as () if eval to false)
    back propagation with each step applying removing unit clause and pure literal
https://en.wikipedia.org/wiki/Conflict-driven_clause_learning#DP_algorithm
Conflict-Driven (CDCL)
    cut implication graph and generate conflict clause. The last "free" decision on tree become forced decision by the conflict clause. Continue.
    unit propagation appear as "forced" decision, do not use pure literal
Look-Ahead
    hueristic selection of decision variable



xor -> addition for binary
and -> multiplication for binary
to simplify determine appearance of coefficient (i.e. if it is non-zero), squash the entire eqn into Z_2 and solve it.

CNF (AND of ORs)

independent set 
    anti clique (no vertex share edge with another vertex in the set) -> the size of a clique < size of independent set partition as one set can contribute at most one vertex to clique

Cube and Conquer
    1. Encode 
    2. Blocked Clause Elimination and Symmetry Breaking
    3. Look ahead (to split to subproblems (a.k.a. cubes))
    4. CDCL to solve the subproblems


chromatic number (min # of color so that adjacent vertices do not have the same color) and graph reduction
Schur Number (min n such that no monochromatic colorings on a+b=c in range)
??? van der Waerden numbers


!!! any existential problems with finitely many possibilities can convert into an SAT encoding
    each object = one clause
    structure inside object -> structure inside clause
    (with extra constraint like more than/less than/exactly)

unavoidable subgraph
    regardless how red/blue assigned, at least one outcome is true






    
    
    
    
    
    
    
    
    
    
    
    



--------------------------commutative algebra---------------------
zero divisors
    nilpotents are zero divisors

nilpotent

integral domain
    can obtain by quotient a prime ideal
    no zero divisors

nilradical/Jacobson radical




ideal quotients

radical

prime ideals
     if a belongs to union ofprime ideals, then
     must belongs to one of them

     
contraction/extension


local ring



prime spectrum
    the inverse image of a prime ideal is prime

irreducible space/irreducible component

induced homomorphism (by spectrum)

algebraic variety

regular mapping


tensor product

module/quotient module
    group with compatible coefficient multiplication
module homomorphism

module annihilator/faithful

direct sum/product of modules
finitely generated

exact sequence
flat
    f: A->B is flat if B is flat as an A-module

algebra
algebra tensor product

??? induced module homomorphism M/aM->N/aN (a ideal)

restriction of scalars

Direct limits
tor
absolutely flat

localization
saturation
total ring of fractions

torsion element
torsion submodule 
torsion-free
faithfully flat
support (of a module)
fiber

restriction homomorphism/presheaf/stalk
constructible topology

primary/p-primary


primary decomposition/minimal/decomposable/belong to/isolated (ideal)/embedded
isolated (set of prime ideals)


saturation (of an ideal)
symbolic power


radical/zero-divisor/nilpotent/primary/p-primary/Primary decomposition/belong to (in module sense)

integral
???faithful module
integral closure
integrally closed in .../integrally closed (without qualification)
integral closure of an ideal

finite separable algebraic extension 
valuation ring 
going-up
going-down

closed mapping
Galois group 
normal extension
algebraically independent

jacobson ring

locally closed

dominate
valuation/value group
isolated subgroup

Noetherian (a.c.c)
    if A is a noetherian ring, then S^{-1}A is noetherian
Artin (d.c.c)

composition series
    maximal chain of submodules
length of a chain
    for vector space with finite length, length = dimension
    
compact (quasi-compact)
???simple modules

irreducible ideals
???dense subset
constructible subsets 
irreducible subset


Grothendieck group

regular ring
    

finite covering

------
radical of an ideal
    taking roots
primary ideal
    !!! (either x in or y^n in) AND (either y in or x^n in) due to commutativity

dimension
    supremum of length of prime ideal chains
    
    dim = 0 equivalent to ???{all prime ideal are maximal} ??? isn't 0 a prime ideal?
    
    will either stay the same or decrease during localization

dim_k
    dimension as k vector space
discrete valuation
    v(1) = 0
    ??? v(unit) = 0? 
        since both unit and its inverse are in the ring and their value are negation of each other
valuation ring
    local domain
    ideal totallly ordered
noetherian
    all ideals finitely generated (i.e. given I, \exists (a_1,...,a_n) = I  
prime ideal
    given a product xy \in I, x \in I or y \in I
    
    0 is a prime ideal, prime ideal never contains unit
PID
    dimension 1 (using contradiction on chain length)

    
torsion-free
    the only torsion element is 0
    local property
flat
    <-> locally free given finitely generated
    [noetherian ring]-module


    
discrete valuation ring (A)
   need local domain, dimension 1, noetherian


field of fractions
    given a ring, its field of fraction is equivalent to (A-0)^{-1}A
dedekind domains
    1. noetherian (local property)
    2. dimension 1
    3. integrally closed (local property)
    <--> discrete valuation ring at any localization
algebraic number field
ring of integers 
fractional ideal/principal fractional ideal
    submodule taken from field of fraction
invertible ideal

integral domain
    local domain = local ring + integral domain
    Noetherian domain = noetherian ring + integral domain

localization A_p
    every ideal contained inside p
quotient A/p
    every ideal containing p (p itself squeeze to 0)

exact sequence
    ker(next) = img(prev)
    
    
    
group of ideals
ideal class group
group of units
content (of a polynomial) 
    ideal generated by all coefficients of the polynomial
    
    if f,g are polynomial, c(fg) \subseteq c(f)c(g)



topological abelian group 
Cauchy sequence
equivalent 
coherent sequence 
inverse system/inverse limits
surjective system
derived functor


\alpha-adic topology (\alpha topology)
topological ring
a-filtration/stable a-filtration
completion/complete/???  [ideal]-adic completions

graded ring
homogeneous (element)/degree/homogeneous components
???G(A)

Zariski ring
???flat extension

additive function
Poincare series
Hilbert function
characteristic polynomial 
    degree -> dimension
height of prime ideal p
    = dim A_p

functor
category


???G_[ideal](A)

coordinate ring
dimension/local dimension of irreducible affine variety


algebraically-closed field
algebraic closure


Grobner basis
    multivariate division order does not matter on this basis
Buchberger’s algorithm
    keep ideal the same, but expand generators so that buchberger criterion is satified
    
    Potential Improvement: select S-pair/reduce S-pair against current basis by multivariate division/update the new pairs

leading monomial/leading term
    term includes both the monomial and the coefficient
    
multivariate division algorithm
    in single variable or plain arithmetic
    dividend = quotient*divisor + remainder
    in multivariate case, divisor are the generators of the ideal
    using leading term to obtain quotient insteads

S-polynomial


polynomial
    the shape is decided by # of terms and # of variables
    
    binomial
        polynomials with at most two terms


# https://askubuntu.com/questions/1058118/help-to-install-macaulay2
Macaulay2
    # download main and common package, use dpkg -i *.deb
    # then apt-get -f install (to finish installing the dependencies)

deepgroebner

    !!! for encoding, each row a pair, suppose l/r each contains m term, each term hard coded n variables (even if they do not appear), total number of encoding per row is then 2*m*n



grobner fan
    given certain groebner basis, produce set of reduced basiss with respect to each term orders, which can in turn produce 1. initial ideals with respect to each term order 2. a univeral grobner basis with respect to all orders (by taking the union)
    polyhedron cone
    face
    polyhedral complex
    fan
    linear form/linear funcional
        linear mapping from a vector space to its field of scalars (in R^n, it can be represented as row vectors (which multiply a column vector and produce a scalar))

convex hull
    smallest convex set (polygon) that contains all the points

plane can be expressed by \vec(a)\vec(x)=b (and scaling \vec(a) and b at the same time will not change the plane), therefore a polytope (polyhedral) can be expressed as Ax <= b after appropriate scaling of each row (face)


??? thereom there are only finitely many distinct initial ideal, therefore finitely many reduced groebner basis

in_w(I) not only introduce new orders (by using the weight vector w), but also covers all the original monomial orders


given a vector w, a polyhedron will produce a face

every polyhedral produces a unique recession cone


??? is a face still a polyhedral?


topological abelian group
    topology + abelian group mappings (i.e. commutative addition and inverse mapping)
    
    
Cauchy sequence
coherent sequence
inverse limit
    G = G_0 and G_n+1 is a subgroup of G_n for any n>=0
    ??? what exactly are the objects in the inverse limit
    completion
        inverse limit of a ring
a-adic topology
    A as G, a^n as G_n (where a is an ideal in A)
filteration
    chain of completion, but using submodule rather than a^nM
    a-filteration
    stable a-filteration
    
    
ideal 
    sum of ideal is still an ideal
    
    
    
complex = polyhedral and their faces
fan = complex in which all polyhedrals are cones
polytope = bounded polyhedron
- is cones still a geometric object (why not a single point)
    extend to inf space by those rays
- equivalence class of weight vectors
    w~v if init_w(I) = init_v(I)

- theorem 2.5 state_d(I) definition
    sum of all exponent vector of degree d then take convex hull

state polytope
    anything that is strongly isomorphic to the original definition state(I)
    coincide with the grobner fan of I
graded ring
    a ring with a graded family (integer index) of subgroups
    
A-module (M) -> AM \in M
p-adic completion (p ideal)
    inverse limit of A/p^k (i.e. A_k = p^k)
G(A) 
    associated graded ring generated from an ideal
finitely generated
    given x \in M, Ax is a submodule of M
    if M can be covered by linear combination of finitely many Ax_i (all x_i \in M), then M is a finitely generated A-module
    
???  since amMn s;; Mm+n

???  a local ring with maximal ideal m
??? Then the kernel E
??? only 10.24/10.25 completion -> original ring


!!! DAMAGED LOCATION
    newton polytope
    can generate universal groebner basis of I directly
affine span (of a polytope)
grobner fan
    ??? equivalent to sets of reduced grobner basis
    is a fan
- normal fan of state polytope coincides with greobner fan
    # equivalent to compute marked reduced groebner basis
dual polyhedron
    take midpoints of edges of each vertex and form a face. THen intersect these faces to get the dual
    
---------------------    
    
    
    
    
    
! 10.4 exactness can translate the property "not a zero divisor"
! 10.5 double bounded inclusion to ensure the same inverse limit (single bound inclusion is not enough)
