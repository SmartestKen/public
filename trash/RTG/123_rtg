So given an ideal with a bunch of generators and a polynomial f, we wish to decide whether f belong to that ideal (i.e. the ideal membership problem.) As what Tyler discussed just now, this reduced to finding a grobner basis for the ideal. The main problem we have in constructing the algorithm is that the usual definition of the grobner basis is not suitabble for actually deciding whether a given basis is grobner or not. 

In order to overcome this, we first introduce the s-polynomial. Given two polynomials and a monomial order, we can construct the s-polynomial as follows. We take the leading monomial of those two polynomials and obtain the lcm, or the least common multiple. Then we simply compute it using this equation, where LT stands for leading term. As you can see, S-polynomial is simply cancelling the leading terms in the two given polynomials. As an example, let's compute the S-polynomial of these two polynomials with the grlex order.

So given the two polynomials, we first obtain their leading monomials under grlex order. Then to visualize how to obtain the least common multiple, we put these row vectors into a matrix and take the column max. Once we obtain the lcm, we plug it into the equation, notice the leading coefficient is included here, and obtain the s polynomial.

Now, buchberger criterion is a criterion for checking grobner basis based on the s-polynomials. Basically if all s-polynomials divided against the basis from which they are derived produce zero remainder, then the basis is grobner. The criterion has a relatively long proof and you can check out the details in the textbook referenced here. Given this criterion, we can construct an iterative algorithm for computing the grobner basis. The idea is the following, given a random set of generators, we obtain all the s-polynomials, select one of them and divide it against this generator set. If it has a non-zero remainder, then we extend the generator set by adding this remainder and update the s-polynomial accordingly. We iterate this procedure till we exhaust all the s-polynomials.

Let's go through the following example. By the way, it is strongly recommended that you use a computer as learning a programming language is trivial comparing to compute a grobner basis out of four generators. At the first iteration, we have the two given generators. Therefore the only s-polynomial we have is the one generated by these two. So we select it, use the divide this s-polynomial against the basis and the remainder is obvious. Since the remainder is non-zero, we add it into the basis.

In the second iteration, the new generator creates a few extra s-polynomials. Again, we select one of them and proceed with the division algorithm with respect to this new basis, which now contains three generators. Since none of the generators' leading terms can divide the leading term of this s-polynomial, the division process is trivial. Again, we update our set of generators as this is another non-zero remainder.

The new generator again adds more s-polynomials. But this time, the s-polynomial we select gives a zero remainder and therefore the basis is not updated. This example is intentionally chosen so that starting from iteration 3, the s polynomials left do not contribute new non-zero remainders. Therefore, we obtain this grobner basis at the end. Usually we do some extra computation to reduce the result to a minimal grobner basis, but let's omit the explanation in this presentation.


To show the correctness of an iterative algorithm, we basically need to show two component. First, the correctness of the previous iteration implies the correctness of current iteration. Second, the algorithm terminates in finitely many iterations. Let's apply this to buchberger algorithm. First, if the set of generators from the previous iteration forms a basis, then the union of a remainder with that generator set becomes the new generator set in current iteration. But by definition of the division algorithm, the remainder belongs to the ideal as well. Hence, the new set is still a basis. It is obvious that the generator set becomes grobner at the end as we exhaust all the s-polynomials. 

For the termination part, as we keep adding the remainders into the generator set, the leading monomial ideal is strictly expanding as the leading monomial of the remainder is not divisible by the leading monomial of any of the existing generators. Hence, by ACC, or ascending chain condition, we can conlcude the algorithm will terminate in finite amount of iterations.


Alright, so we have the algorithm and it is correct. But it is by no means efficient. The complexity of this algorithm increases as a double exponential of the number of variables. There were many attempts to make the algorithm more efficient. Let me introduce you an interesting improvement based on reinforcement learning. 

From a high level point of view, a reinforcemnet learning algorithm consists of three components, states, actions and rewards. At a given state, the agent, or the algorithm, selects an action and execute and receives a reward from the environment. The resulting states are updated accordingly after receiving the reward. This looks very similar to the structure of the buchberger algorithm and we can formulate it into a rreinforcement learning problem. The states, in this case, are all the generators and their s-polynomials that do not go through the division yet. The set of actions are basically selecting one of the s-polynomial. The reward in this case is the negative of the number of operations. In other words, the agent is penalized for making more computations. 

The paper mainly focus on improving the selection process and uses the so-called normal selection strategy as a benchmark. In this empirical strategy, we always select the s-polynomial whose generators produce the smallest lcm. While the paper makes lots of compromises in order to train the algorithm in neural networks, such as restricting to the case of bionomials and using a large finite field to simulate the characteristic zero field, its result is non-trivial. As you can see here, it achieves approximately 20% computation reduction comparing to the normal strategy. 

Ok. That's all I want to talk about today. Thank you.

